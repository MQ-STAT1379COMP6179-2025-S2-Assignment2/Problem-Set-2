---
title: "Problem Set 2"
author: Austin Borg
bibliography: references.bib
format: html
editor: visual
---

## Exercise 1 - PISA 2022 Visualisations

```{r pisa-plot1,fig.width=7,fig.height=5}

#-------
# Plot 1
#-------

library(tidyverse)
library(knitr)
library(here)
library(ggrepel)

# Loading Dataset
pisa <- read.csv("pisa_2022.csv")

# Summarising Averages by Country
pisa_summary <- pisa%>%
group_by(country) %>%
  summarise(
    mean_math = mean(math, na.rm = TRUE),
    mean_read = mean(read,na.rm = TRUE)
  )

# Creating Scatterplot
ggplot(pisa_summary,
aes(x = mean_math, y = mean_read, label = country)) +
geom_point(size = 2) +
geom_label_repel(
size = 3,
label.size = 0.4,
color = "black",
fill = "white",
segment.color = "black",
max.overlaps = 30
) +
labs(
title = "Average reading against average maths for each country",
subtitle = "Any guesses on where Australia ranks?",
x = "mean scores in mathematics",
y = "mean scores in reading"
) +
theme_minimal(base_size = 12)
```

```{r pisa-plot2,fig.width=7,fig.height=5}

#-------
# Plot 2
#-------

# Load and summarise again (for clarity)
pisa <- read_csv(here("pisa_2022.csv"))

pisa_gender <- pisa %>%
  group_by(country, gender) %>%
  summarise(
    mean_math = mean(math, na.rm = TRUE),
    mean_read = mean(read, na.rm = TRUE),
    mean_sci  = mean(science, na.rm = TRUE)
  )

# Compute female - male differences
pisa_diff <- pisa_gender %>%
  pivot_wider(names_from = gender, values_from = c(mean_math, mean_read, mean_sci)) %>%
  mutate(
    diff_math = mean_math_female - mean_math_male,
    diff_read = mean_read_female - mean_read_male,
    diff_sci  = mean_sci_female - mean_sci_male
  )

# Reshape to long format for plotting
pisa_long <- pisa_diff %>%
  select(country, diff_math, diff_read, diff_sci) %>%
  pivot_longer(
    cols = starts_with("diff_"),
    names_to = "subject",
    values_to = "diff"
  ) %>%
  mutate(
    subject = recode(subject,
                     diff_math = "mean_math",
                     diff_read = "mean_read",
                     diff_sci  = "mean_sci"),
    diff_sign = diff > 0  # TRUE if girls higher, FALSE if boys higher
  )

# Create Plot
ggplot(pisa_long, aes(x = diff, y = country, color = diff_sign)) +
  geom_point(size = 2) +
  geom_vline(xintercept = 0, color = "black", linewidth = 0.8) +
  facet_wrap(~subject, ncol = 3) +
  scale_color_manual(values = c("FALSE" = "#F8766D", "TRUE" = "#00BFC4")) +
  labs(
    title = "Average gender difference (diff = female - male) per Country",
    subtitle = "Gender gap in reading is universal, but math and science gaps are not.",
    x = "diff",
    y = "country",
    color = "diff > 0"
  ) +
  scale_x_continuous(limits = c(-60, 60)) + 
  theme_minimal(base_size = 12) +
  annotate("text", x = -50, y = 2, label = "Boys", 
           size = 3, fontface = "bold", color = "black") +
  annotate("text", x =  50, y = 2, label = "Girls", 
           size = 3, fontface = "bold", color = "black") +
  theme(
    panel.background = element_rect(fill = "grey95", color = NA),
    strip.background = element_rect(fill = "grey85", color = NA),
    strip.text = element_text(face = "bold"),
    axis.text.y = element_text(size = 6),
    legend.position = "right"
  )

```

## Exercise 2 - GDP Plots

```{r plot 1, fig.width=7, fig.height=5}
library(tidyverse)
library(dplyr)
library(here)
library(zoo)
library(ggplot2)

#------
#Plot 1
# -----

# Load Datasets

# Reading Headers from Metadata
header_house <- read_csv(here("aus_median_house_price.csv"), n_max = 0)
col_names <- names(header_house)

# Reading data from beginning of real data
house <- read_csv(here("aus_median_house_price.csv"), skip = 10, col_names = FALSE)

# Give data correct headers
names(house) <- col_names

names(house)
# Extract only Melbourne Data
melbourne <- house %>%
  rename(Quarter = ...1) %>% # rename variable to Quarter
  select(
    Quarter,
    Melbourne = 'Median Price of Established House Transfers (Unstratified) ;  Melbourne ;' #Rename Melbourne header
  )

# Clean values by converting Quarter to Date value (Year) and Melbourne price to numeric
melbourne <- melbourne %>%
  mutate(
    Quarter = as.yearqtr(Quarter, format = "%b-%Y"),
    Melbourne = as.numeric(Melbourne)
  )

# Plotting clean Melbourne data

ggplot(melbourne, aes(x = Quarter, y = Melbourne)) +
  geom_line(color = "blue", size = 0.75) +          # Line for Melbourne prices
  scale_x_yearqtr(format = "%Y") +          # Format x-axis as Year
  scale_y_continuous(
    breaks = seq(300, 900, by = 100),
  ) +
  labs(
    title = "Median Price of Established House Transfers in Melbourne",
    subtitle = "Unstratified & not seasonally adjusted",
    x = " ",
    y = "A$ ('000)",
    caption = "Source: ABS"
  ) +
  theme(
    panel.background = element_rect(fill = "grey90"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "white"),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(angle = 0, hjust = 0.5),  # Flat x-axis labels
    plot.caption = element_text(hjust = 1, size = 10) # add source
  )

```

```{r plot 2, fig.width=7, fig.height=5}
library(tidyverse)
library(dplyr)
library(here)
library(zoo)
library(ggplot2)


#-------
# Plot 2 
#-------

# --- GDP data ---
gdp <- read.csv("aus_GDP_changes.csv")

# Convert DATE to proper format and create Quarter & Low_GDP
gdp <- gdp %>%
  mutate(
    DATE = as.Date(DATE, format = "%d/%m/%Y"),
    Quarter = as.yearqtr(DATE),
    Low_GDP = GDP_changes < 0.1
  )

# Filter GDP to Melbourne date range
min_q <- min(melbourne$Quarter)
max_q <- max(melbourne$Quarter)

gdp_filtered <- gdp %>%
  filter(Quarter >= min_q & Quarter <= max_q)

# Merge consecutive low-GDP quarters into shaded blocks
shades <- gdp_filtered %>%
  filter(Low_GDP) %>%
  # Create xmin/xmax for each quarter individually
  mutate(
    xmin = Quarter - 0.125,
    xmax = Quarter + 0.125,
    ymin = -Inf,
    ymax = Inf
  )

# Plot
ggplot(melbourne, aes(x = Quarter, y = Melbourne)) +
  geom_rect(
    data = shades, 
    aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax),
    inherit.aes = FALSE,
    fill = "grey70",
    alpha = 0.3
  ) +
  geom_line(color = "blue", size = 0.75) +
  scale_y_continuous(
    breaks = seq(300, 900, by = 100)
  ) +
  scale_x_yearqtr(format = "%Y") +
  labs(
    title = "Median Price of Established House Transfers in Melbourne",
    subtitle = "Unstratified & not seasonally adjusted",
    x = " ",
    y = "A$ ('000)",
    caption = "Shaded areas indicate GDP increased less than 0.1% at that quarter\nSource: ABS"
  ) +
  theme(
    panel.background = element_rect(fill = "white"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y = element_line(color = "grey70"),
    panel.grid.minor.y = element_blank(),
    axis.text.x = element_text(angle = 0, hjust = 0.5),  # Flat x-axis labels
    plot.caption = element_text(hjust = 1, size = 10) # add caption
  )

```

## Exercise 3 - Movies Database

```{r}
library(tidyverse)
library(DBI)
library(duckdb)
library(dbplyr)
library(here)

#------------------------------
# Creating Database from Week 7
#------------------------------

# Create a DuckDB connection
con <- dbConnect(duckdb::duckdb())

# Read and load the CSV files into DuckDB
actors <- read_csv(here("data", "Actors.csv"))
castings <- read_csv(here("data", "Castings.csv"))
directors <- read_csv(here("data", "Directors.csv"))
movies <- read_csv(here("data", "Movies.csv"))
movies_and_directors <- read_csv(here("data", "MoviesAndDirectors.csv"))

# Copy data frames to DuckDB
dbWriteTable(con, "actors", actors)
dbWriteTable(con, "castings", castings)
dbWriteTable(con, "directors", directors)
dbWriteTable(con, "movies", movies)
dbWriteTable(con, "movies_and_directors", movies_and_directors)

# Create dplyr table references
actors_db <- tbl(con, "actors")
castings_db <- tbl(con, "castings")
directors_db <- tbl(con, "directors")
movies_db <- tbl(con, "movies")
movies_directors_db <- tbl(con, "movies_and_directors")

#-------------------------------------------------
# Part A - Most Common Director-Actor Partnerhsips
#-------------------------------------------------

partnerships <- castings_db %>%
  left_join(actors_db, by = "ActorID") %>%
  left_join(movies_db, by = "MovieID") %>%
  left_join(directors_db, by = "DirectorID") %>% # joins all relevent tables
  group_by(Director = Name.y, Actor = Name.x) %>%  # group by both names
  summarise(num_movies_together = n()) %>% # summarises partnerships
  arrange(desc(num_movies_together)) # arranges by most common partnerships

# Top 10 most common partnerships
head(partnerships, 10) 

#------------------------------
# Part B - Translating to dbplyr
#------------------------------

actor_summary <- actors_db %>%
  left_join(castings_db, by = "ActorID") %>%
  left_join(movies_db, by = "MovieID") %>%
  left_join(directors_db, by = "DirectorID") %>%
  group_by(ActorName = Name.x) %>%                   # group by actor
  summarise(
    unique_directors = n_distinct(Name.y),           # count distinct directors
    total_movies = n(),                              # count all movies
    .groups = "drop"
  ) %>%
  arrange(desc(unique_directors))                    # order by most versatile actors

# View the top 10 actors
head(actor_summary, 10)
```

The above code in Part B, translated to dbplyr, reflects how many different directors each actor has worked with (unique_directors), alongside the total amount of movies they have appeared in (total_movies). This query may have been motivated by someone looking to find which actors have worked with the most different directors, or alternatively which actors have appeared in the most movies.

## Exercise 4

```{r}
library(dplyr)      
library(tidyr)      
library(ggplot2)    

#-----------------------------------------------------------
# Step 1 - Define function to simulate 2x2 contingency table
#-----------------------------------------------------------

simulate_table <- function(n_men, n_women, success_prob) {
  repeat {  # Ensures table never has zero studying individuals
    # Simulate number of studying men and women
    studying_men <- rbinom(1, n_men, success_prob)
    studying_women <- rbinom(1, n_women, success_prob)
    
    # Ensure at least one studying individual
    if ((studying_men + studying_women) > 0) break
  }
  
  # 2x2 Contingency table
  table <- matrix(
    c(studying_men,
      studying_women,
      n_men - studying_men,
      n_women - studying_women),
    nrow = 2,
    byrow = TRUE
  )
  rownames(table) <- c("Studying", "NotStudying")
  colnames(table) <- c("Men", "Women")
  
  return(table)
}


#-------------------------------------------
# Step 2 - Define function to run simulation
#-------------------------------------------

run_simulation <- function(n_iter, n_men, n_women, success_prob) {
  p_chisq <- numeric(n_iter)   # Store chi-squared p-values
  p_fisher <- numeric(n_iter)  # Store Fisher p-values
  
  for (i in 1:n_iter) {        # Simulates a new table each time
    tab <- simulate_table(n_men, n_women, success_prob)
    
    # Chi-squared test with Yates correction
    test_chisq <- suppressWarnings(chisq.test(tab, correct = TRUE))
    p_chisq[i] <- test_chisq$p.value
    
    # Fisher's exact test
    test_fisher <- fisher.test(tab)
    p_fisher[i] <- test_fisher$p.value
  }
  
  return(list(chi_sq = p_chisq, fisher = p_fisher))
}


#-----------------------------------
# Step 3 - Define simulation setting
#-----------------------------------

#Produce 4 different scenarios
settings <- list(
  # Small sample, low success prob
  small_small  = list(n_men = 20, n_women = 20, prob = 0.2),
  # Small sample, normal success prob
  small_normal = list(n_men = 20, n_women = 20, prob = 0.5),
  # Large sample, low success prob
  large_small  = list(n_men = 100, n_women = 100, prob = 0.2),
  # Large sample, normal success prob
  large_normal = list(n_men = 100, n_women = 100, prob = 0.5)
)


#------------------------
# Step 4 - Run Simulations
#------------------------

set.seed(123)      # ensures reproducibility
n_iter <- 10000    # 10000 contingency tables per setting

# Simulates all 4 scenarios, with chisq and fisher's exact for each
results <- lapply(settings, function(s) {
  run_simulation(n_iter, s$n_men, s$n_women, s$prob)
})


#--------------------------------------------------------------------
# Step 5 - Calculate Rejection Rates at different significance levels
#--------------------------------------------------------------------

alpha_levels <- c(0.10, 0.05, 0.01)

rejection_rates <- lapply(results, function(res) {
  sapply(alpha_levels, function(alpha) {
    c(
      chisq  = mean(res$chi_sq < alpha),
      fisher = mean(res$fisher < alpha)
    )
  })
})

rejection_rates
```

#### Analysing Rejection Rates:

In both the chi-squared and Fisher's exact tests, rejection rates are conservative with smaller sample sizes and lower probabilities. As these increase, the rejection rates tend to become less conservative, climbing closer to the nominal alpha level. Additionally, lower nominal alpha levels tend to produce more conservative results, especially in smaller sample sizes and lower probabilities. This is due to smaller expected counts limiting the p-values. Overall, Fisher's exact test is consistently valid but slightly conservative in smaller tables, whereas the chi-squared test relies on larger expected counts to more accurately reflect the nominal alpha.
